{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['PL', 'STC', 'CC', 'HLNC', 'OBNC', 'BE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ['adult_sex_Male', 'adult_race_White', 'german_sex_Male', 'compas_sex', 'compas_race_Caucasian', 'ricci_Race_W', 'diabetes_race_Caucasian', 'titanic_sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {}\n",
    "for exp in experiments:\n",
    "    for alg in algorithms:\n",
    "        run = mlflow.search_runs(experiment_names=[f'{exp}_{alg}'], order_by=['start_time DESC'])[:4]\n",
    "        if len(run) > 0:\n",
    "            runs[f'{exp}_{alg}'] = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'accuracy',\n",
    "    'roc_auc',\n",
    "    'equal_opportunity_difference', \n",
    "    'predictive_equality_difference',\n",
    "    'demographic_parity_difference',\n",
    "    'equalized_odds_difference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(test_set, experiments):\n",
    "    results = {}\n",
    "\n",
    "    for alg in algorithms:\n",
    "        results_df = pd.DataFrame(columns=metrics)\n",
    "\n",
    "        for exp in experiments:\n",
    "            if f'{exp}_{alg}' not in runs:\n",
    "                print(f'{exp}_{alg} must be repeated')\n",
    "                continue\n",
    "\n",
    "            run = runs[f'{exp}_{alg}']\n",
    "            noisy = run.loc[(run['tags.train_set'] == 'noisy') & (run['tags.test_set'] == test_set)]\n",
    "            corrected = run.loc[(run['tags.train_set'] == 'corrected') & (run['tags.test_set'] == test_set)]\n",
    "\n",
    "            row = []\n",
    "            for metric in metrics:\n",
    "                if noisy[f'metrics.{metric}'].values[0] == 0:\n",
    "                    row.append(0)\n",
    "                else:\n",
    "                    if metric == 'accuracy' or metric == 'roc_auc':\n",
    "                        row.append((corrected[f'metrics.{metric}'].values[0] - noisy[f'metrics.{metric}'].values[0])/noisy[f'metrics.{metric}'].values[0])\n",
    "                    else:\n",
    "                        row.append((noisy[f'metrics.{metric}'].values[0] - corrected[f'metrics.{metric}'].values[0])/noisy[f'metrics.{metric}'].values[0])\n",
    "            results_df.loc[exp] = row\n",
    "            \n",
    "        results[alg] = results_df\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results_by_alg(results, path):\n",
    "    if not os.path.exists(f'{path}/correction alg results'):\n",
    "        os.mkdir(f'{path}/correction alg results')\n",
    "        \n",
    "    for alg in algorithms:\n",
    "        results[alg].to_csv(f'{path}/correction alg results/{alg}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_aggregated_results(results, path):\n",
    "    if not os.path.exists(f'{path}/aggregated results'):\n",
    "        os.mkdir(f'{path}/aggregated results')\n",
    "        \n",
    "    for metric in metrics:\n",
    "        agg_results = pd.DataFrame(columns=['mean', 'std', 'min', 'max'])\n",
    "        for alg in algorithms:\n",
    "            agg_results.loc[alg] = results[alg][metric].describe()[['mean', 'std', 'min', 'max']].values\n",
    "        agg_results.to_csv(f'{path}/aggregated results/{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    for op in ['mean', 'std', 'min', 'max']:\n",
    "        df = pd.DataFrame(columns=metrics)\n",
    "        for alg in algorithms:\n",
    "            df.loc[alg] = [results[alg][metric].describe()[op] for metric in metrics]\n",
    "        df.to_csv(f'{path}/{op}_results.csv')\n",
    "    \n",
    "    store_results_by_alg(results, path)\n",
    "    store_aggregated_results(results, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('fairness_results'):\n",
    "    os.mkdir('fairness_results')\n",
    "\n",
    "for test_set in ['noisy', 'corrected']:\n",
    "    path = f'fairness_results/{test_set} test set'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    results_fair = get_results(test_set, experiments)\n",
    "    store_results(results_fair, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
