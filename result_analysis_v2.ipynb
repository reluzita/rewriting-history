{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['PL', 'STC', 'CC', 'HLNC', 'OBNC', 'BE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {}\n",
    "for dataset in ['adult', 'german', 'compas', 'ricci', 'diabetes']:\n",
    "    for attr in ['sex_Male', 'race_White', 'sex', 'race_Caucasian', 'Race_W']:\n",
    "        for alg in algorithms:\n",
    "            run = mlflow.search_runs(experiment_names=[f'{dataset}_{attr}_{alg}'], order_by=['start_time DESC'])[:4]\n",
    "            if len(run) > 0:\n",
    "                runs[f'{dataset}_{attr}_{alg}'] = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'accuracy',\n",
    "    'equal_opportunity_difference', \n",
    "    'predictive_equality_difference',\n",
    "    'demographic_parity_difference',\n",
    "    'equalized_odds_difference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for alg in algorithms:\n",
    "    results_df = pd.DataFrame(columns=metrics)\n",
    "\n",
    "    for exp in ['adult_sex_Male', 'adult_race_White', 'german_sex_Male', 'compas_sex', 'compas_race_Caucasian', 'ricci_Race_W', 'diabetes_race_Caucasian']:\n",
    "        if f'{exp}_{alg}' not in runs:\n",
    "            print(f'{exp}_{alg} must be repeated')\n",
    "            continue\n",
    "        run = runs[f'{exp}_{alg}']\n",
    "        noisy = run.loc[(run['tags.train_set'] == 'noisy') & (run['tags.test_set'] == 'noisy')]\n",
    "        corrected = run.loc[(run['tags.train_set'] == 'corrected') & (run['tags.test_set'] == 'noisy')]\n",
    "\n",
    "        results_df.loc[exp] = [\n",
    "            (corrected[f'metrics.{metric}'].values[0] - noisy[f'metrics.{metric}'].values[0])/noisy[f'metrics.{metric}'].values[0] \n",
    "            if noisy[f'metrics.{metric}'].values[0] != 0\n",
    "            else 0 \n",
    "            for metric in metrics]\n",
    "        \n",
    "    results[alg] = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg in algorithms:\n",
    "    results[alg].to_csv(f'results/correction_alg_results/{alg}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    results_df = pd.DataFrame(columns=['mean', 'std', 'min', 'max'])\n",
    "    for alg in algorithms:\n",
    "        results_df.loc[alg] = results[alg][metric].describe()[['mean', 'std', 'min', 'max']].values\n",
    "    results_df.to_csv(f'results/aggregated_results/{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=metrics)\n",
    "for alg in algorithms:\n",
    "    results_df.loc[alg] = [results[alg][metric].describe()['mean'] for metric in metrics]\n",
    "results_df.to_csv(f'results/aggregated_results/mean_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
